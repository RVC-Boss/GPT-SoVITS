#!/usr/bin/env python3
"""
GPT-SoVITS only_tts ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸

V4 ë° V2Pro ì‹œë¦¬ì¦ˆ ëª¨ë¸ë“¤ì„ ìë™ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ê³  ì„¤ì¹˜í•©ë‹ˆë‹¤.

Usage:
    python download_models.py --all
    python download_models.py --v4
    python download_models.py --v2pro
    python download_models.py --base-models
"""

import os
import sys
import argparse
import subprocess
import shutil
from pathlib import Path
import requests
from tqdm import tqdm
import zipfile
import hashlib

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ pretrained_models ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).parent
PRETRAINED_DIR = SCRIPT_DIR / "pretrained_models"

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì •ë³´
MODEL_CONFIGS = {
    "base_models": {
        "chinese-hubert-base": {
            "url": "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/chinese-hubert-base.zip",
            "path": PRETRAINED_DIR / "chinese-hubert-base",
            "description": "Multi-language HuBERT base model (í•œêµ­ì–´/ì˜ì–´ í•„ìˆ˜ - ë‹¤êµ­ì–´ ìŒì„± íŠ¹ì§• ì¶”ì¶œ)"
        },
        "chinese-roberta-wwm-ext-large": {
            "url": "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/chinese-roberta-wwm-ext-large.zip",
            "path": PRETRAINED_DIR / "chinese-roberta-wwm-ext-large",
            "description": "Multi-language RoBERTa model (í•œêµ­ì–´/ì˜ì–´ í•„ìˆ˜ - ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ íŠ¹ì§• ì¶”ì¶œ)"
        }
    },
    "v4_models": {
        "s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt": {
            "url": "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/gsv-v4-pretrained/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt",
            "path": PRETRAINED_DIR / "gsv-v4-pretrained" / "s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt",
            "description": "V4 GPT model checkpoint"
        },
        "s2Gv4.pth": {
            "url": "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/gsv-v4-pretrained/s2Gv4.pth",
            "path": PRETRAINED_DIR / "gsv-v4-pretrained" / "s2Gv4.pth",
            "description": "V4 SoVITS model"
        },
        "vocoder.pth": {
            "url": "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/gsv-v4-pretrained/vocoder.pth",
            "path": PRETRAINED_DIR / "gsv-v4-pretrained" / "vocoder.pth",
            "description": "V4 Vocoder model"
        }
    },
    "v2pro_models": {
        "s1bert25hz-5kh-longer-epoch=12-step=369668.ckpt": {
            "url": "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/v2Pro/s1bert25hz-5kh-longer-epoch=12-step=369668.ckpt",
            "path": PRETRAINED_DIR / "v2Pro" / "s1bert25hz-5kh-longer-epoch=12-step=369668.ckpt",
            "description": "V2Pro GPT model checkpoint"
        },
        "s2Gv2Pro.pth": {
            "url": "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/v2Pro/s2Gv2Pro.pth",
            "path": PRETRAINED_DIR / "v2Pro" / "s2Gv2Pro.pth",
            "description": "V2Pro SoVITS model"
        },
        "s2Gv2ProPlus.pth": {
            "url": "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/v2Pro/s2Gv2ProPlus.pth",
            "path": PRETRAINED_DIR / "v2Pro" / "s2Gv2ProPlus.pth",
            "description": "V2ProPlus SoVITS model"
        },
        "pretrained_eres2netv2w24s4ep4.ckpt": {
            "url": "https://huggingface.co/lj1995/GPT-SoVITS/resolve/main/sv/pretrained_eres2netv2w24s4ep4.ckpt",
            "path": PRETRAINED_DIR / "sv" / "pretrained_eres2netv2w24s4ep4.ckpt",
            "description": "Speaker Verification model (V2Pro í•„ìˆ˜)"
        }
    }
}

def download_file(url: str, filepath: Path, description: str = ""):
    """íŒŒì¼ ë‹¤ìš´ë¡œë“œ (ì§„í–‰ë¥  í‘œì‹œ)"""
    try:
        # HEAD ìš”ì²­ìœ¼ë¡œ íŒŒì¼ í¬ê¸° í™•ì¸
        response = requests.head(url, allow_redirects=True)
        total_size = int(response.headers.get('content-length', 0))

        # ë””ë ‰í† ë¦¬ ìƒì„±
        filepath.parent.mkdir(parents=True, exist_ok=True)

        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        response = requests.get(url, stream=True)
        response.raise_for_status()

        with open(filepath, 'wb') as f:
            with tqdm(
                desc=f"Downloading {description or filepath.name}",
                total=total_size,
                unit='B',
                unit_scale=True,
                unit_divisor=1024,
            ) as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))

        print(f"âœ… Downloaded: {filepath}")
        return True

    except Exception as e:
        print(f"âŒ Failed to download {url}: {e}")
        return False

def check_file_exists(filepath: Path) -> bool:
    """íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
    return filepath.exists() and filepath.stat().st_size > 0

def download_models(model_groups: list, force: bool = False):
    """ëª¨ë¸ ë‹¤ìš´ë¡œë“œ"""
    print(f"ğŸš€ GPT-SoVITS only_tts ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘")
    print(f"ğŸ“ ì„¤ì¹˜ ê²½ë¡œ: {PRETRAINED_DIR.absolute()}")

    total_files = 0
    downloaded_files = 0
    skipped_files = 0

    for group in model_groups:
        if group not in MODEL_CONFIGS:
            print(f"âš ï¸  ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ ê·¸ë£¹: {group}")
            continue

        print(f"\nğŸ“¦ {group.replace('_', ' ').title()} ë‹¤ìš´ë¡œë“œ ì¤‘...")
        models = MODEL_CONFIGS[group]

        for model_name, model_info in models.items():
            total_files += 1
            filepath = model_info["path"]

            # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ê³  forceê°€ ì•„ë‹Œ ê²½ìš° ìŠ¤í‚µ
            if check_file_exists(filepath) and not force:
                print(f"â­ï¸  ì´ë¯¸ ì¡´ì¬í•¨: {filepath.name}")
                skipped_files += 1
                continue

            # ë‹¤ìš´ë¡œë“œ ì‹œë„
            if download_file(model_info["url"], filepath, model_info["description"]):
                downloaded_files += 1
            else:
                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {model_name}")

    # ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“Š ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!")
    print(f"   ì´ íŒŒì¼: {total_files}")
    print(f"   ë‹¤ìš´ë¡œë“œ: {downloaded_files}")
    print(f"   ìŠ¤í‚µ: {skipped_files}")

    if downloaded_files > 0:
        print(f"\nâœ… ìƒˆë¡œ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ë“¤ì´ {PRETRAINED_DIR} ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def install_git_lfs():
    """Git LFS ì„¤ì¹˜ í™•ì¸ ë° ì„¤ì¹˜"""
    try:
        subprocess.run(["git", "lfs", "--version"], check=True, capture_output=True)
        print("âœ… Git LFSê°€ ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âš ï¸  Git LFSê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

        # ìë™ ì„¤ì¹˜ ì‹œë„ (Linux/Mac)
        if sys.platform != "win32":
            try:
                if shutil.which("apt-get"):  # Ubuntu/Debian
                    subprocess.run(["sudo", "apt-get", "install", "-y", "git-lfs"], check=True)
                elif shutil.which("brew"):  # macOS
                    subprocess.run(["brew", "install", "git-lfs"], check=True)
                elif shutil.which("yum"):  # CentOS/RHEL
                    subprocess.run(["sudo", "yum", "install", "-y", "git-lfs"], check=True)

                subprocess.run(["git", "lfs", "install"], check=True)
                print("âœ… Git LFS ì„¤ì¹˜ ì™„ë£Œ!")
                return True
            except subprocess.CalledProcessError:
                pass

        print("âŒ Git LFSë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”: https://git-lfs.github.io/")
        return False

def check_dependencies():
    """ì˜ì¡´ì„± í™•ì¸"""
    print("ğŸ” ì˜ì¡´ì„± í™•ì¸ ì¤‘...")

    # Python íŒ¨í‚¤ì§€ í™•ì¸
    required_packages = ["requests", "tqdm"]
    missing_packages = []

    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)

    if missing_packages:
        print(f"âš ï¸  í•„ìš”í•œ íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤: {', '.join(missing_packages)}")
        print(f"   ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install {' '.join(missing_packages)}")
        return False

    print("âœ… ì˜ì¡´ì„± í™•ì¸ ì™„ë£Œ!")
    return True

def show_model_info():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì •ë³´ í‘œì‹œ"""
    print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë“¤:")
    print()

    for group_name, models in MODEL_CONFIGS.items():
        print(f"ğŸ”¸ {group_name.replace('_', ' ').title()}:")
        for model_name, model_info in models.items():
            status = "âœ…" if check_file_exists(model_info["path"]) else "âŒ"
            print(f"   {status} {model_name}")
            print(f"      ğŸ“ {model_info['description']}")
            print(f"      ğŸ“ {model_info['path']}")
        print()

def main():
    parser = argparse.ArgumentParser(
        description="GPT-SoVITS only_tts ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  python download_models.py --all           # ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
  python download_models.py --v4            # V4 ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ
  python download_models.py --v2pro         # V2Pro ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ
  python download_models.py --base-models   # ê¸°ë³¸ ëª¨ë¸ë§Œ ë‹¤ìš´ë¡œë“œ
  python download_models.py --info          # ëª¨ë¸ ì •ë³´ í‘œì‹œ
        """
    )

    parser.add_argument("--all", action="store_true", help="ëª¨ë“  ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--v4", action="store_true", help="V4 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--v2pro", action="store_true", help="V2Pro ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--base-models", action="store_true", help="ê¸°ë³¸ ëª¨ë¸ë“¤ ë‹¤ìš´ë¡œë“œ")
    parser.add_argument("--force", action="store_true", help="ê¸°ì¡´ íŒŒì¼ ë®ì–´ì“°ê¸°")
    parser.add_argument("--info", action="store_true", help="ëª¨ë¸ ì •ë³´ í‘œì‹œ")

    args = parser.parse_args()

    # ì •ë³´ í‘œì‹œ
    if args.info:
        show_model_info()
        return

    # ì˜ì¡´ì„± í™•ì¸
    if not check_dependencies():
        sys.exit(1)

    # ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ê·¸ë£¹ ê²°ì •
    model_groups = []

    if args.all:
        model_groups = ["base_models", "v4_models", "v2pro_models"]
    else:
        if args.base_models:
            model_groups.append("base_models")
        if args.v4:
            model_groups.append("base_models")  # V4ëŠ” ê¸°ë³¸ ëª¨ë¸ í•„ìš”
            model_groups.append("v4_models")
        if args.v2pro:
            model_groups.append("base_models")  # V2Proë„ ê¸°ë³¸ ëª¨ë¸ í•„ìš”
            model_groups.append("v2pro_models")

    # ì•„ë¬´ ì˜µì…˜ë„ ì„ íƒí•˜ì§€ ì•Šì€ ê²½ìš°
    if not model_groups:
        print("â“ ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        print("   --help ì˜µì…˜ìœ¼ë¡œ ì‚¬ìš©ë²•ì„ í™•ì¸í•˜ì„¸ìš”.")
        print()
        show_model_info()
        return

    # ì¤‘ë³µ ì œê±°
    model_groups = list(dict.fromkeys(model_groups))

    # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
    download_models(model_groups, args.force)

    # ì„¤ì¹˜ ì™„ë£Œ ë©”ì‹œì§€
    print(f"\nğŸ‰ ëª¨ë¸ ì„¤ì¹˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"   ì´ì œ tts_simple.pyë¥¼ ì‚¬ìš©í•˜ì—¬ TTSë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print()
    print("ğŸ“– ì‚¬ìš© ì˜ˆì‹œ:")
    print("   from tts_simple import TTSEngine")
    print("   tts = TTSEngine(model='v4', device='cuda')")

if __name__ == "__main__":
    main()
